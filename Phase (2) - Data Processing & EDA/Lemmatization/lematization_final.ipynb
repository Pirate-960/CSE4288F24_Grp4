{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Dataset: 100%|██████████| 120/120 [23:47<00:00, 11.90s/it] \n",
      "Processing Dataset: 100%|██████████| 7/7 [01:59<00:00, 17.08s/it] \n",
      "Processing Dataset: 100%|██████████| 7/7 [02:00<00:00, 17.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed in 1671.29 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from zeyrek import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Configure logging (minimal logging for performance)\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename='process_preprocessing_v3.log',\n",
    "    filemode='w',  # Overwrite the log file each run\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.FATAL\n",
    ")\n",
    "\n",
    "# Load NLTK Turkish stop words\n",
    "try:\n",
    "    nltk_stop_words = stopwords.words('turkish')\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk_stop_words = stopwords.words('turkish')\n",
    "\n",
    "# Custom stop words\n",
    "custom_stop_words = [\n",
    "    'karar', 'mahkemesi', 'edildiği', 'şöyledir', 'hakkının', 'hukuk',\n",
    "    'hakkında', 'ifade', 'tarihli', 'olarak', 'ceza', 'kararı', 'başvuru',\n",
    "    'başvurucunun', 'üzerine', 'yapılan', 'tarafından', 'kabul', 'verilmiştir',\n",
    "    'kanun', 'tarihinde', 'dava', 'nedeniyle', 'ilişkin', 'maddesinin', 'ilgili',\n",
    "    'başvurucu', 'yer', 'terör', 'sayılı', 'olan', 'ihlal', 'olduğu', 'nin'\n",
    "]\n",
    "\n",
    "# Combine stop words and compile regex\n",
    "all_stop_words = set(nltk_stop_words + custom_stop_words)\n",
    "stop_words_regex = re.compile(r'\\b(?:' + '|'.join(map(re.escape, all_stop_words)) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Global MorphAnalyzer for worker processes\n",
    "analyzer = None\n",
    "\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json_file(data, file_path):\n",
    "    \"\"\"Save JSON file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans the text by removing punctuation, digits, stop words, and converting to lowercase.\"\"\"\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove digits and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = stop_words_regex.sub('', text)  # Remove stop words\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatizes a single text.\"\"\"\n",
    "    global analyzer\n",
    "    cleaned_text = clean_text(text)\n",
    "    words = cleaned_text.split()\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemmas = analyzer.lemmatize(word)\n",
    "        if lemmas:\n",
    "            lemmatized_words.append(lemmas[0][1][0])  # Append the first lemma\n",
    "        else:\n",
    "            lemmatized_words.append(word)  # Fallback to the original word\n",
    "    #print(' '.join(lemmatized_words))\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "def process_texts_batch(batch):\n",
    "    \"\"\"Processes a batch of texts for lemmatization using the global analyzer.\"\"\"\n",
    "    processed_batch = []\n",
    "    for item in batch:\n",
    "        processed_item = {}\n",
    "        for field in ['text', 'Başvuru Konusu']:\n",
    "            processed_item[field] = lemmatize_text(item[field])\n",
    "        processed_batch.append(processed_item)\n",
    "    return processed_batch\n",
    "\n",
    "\n",
    "def worker_init():\n",
    "    \"\"\"Initializes the global MorphAnalyzer in workers.\"\"\"\n",
    "    global analyzer\n",
    "    analyzer = MorphAnalyzer()\n",
    "\n",
    "\n",
    "def process_dataset_in_batches(dataset, batch_size=100):\n",
    "    \"\"\"Processes the dataset in batches using multiprocessing.\"\"\"\n",
    "    with Pool(processes=cpu_count(), initializer=worker_init) as pool:\n",
    "        # Split dataset into batches\n",
    "        batches = [dataset[i:i + batch_size] for i in range(0, len(dataset), batch_size)]\n",
    "        \n",
    "        # Parallel processing\n",
    "        results = list(tqdm(pool.imap(process_texts_batch, batches),\n",
    "                            total=len(batches), desc=\"Processing Dataset\"))\n",
    "\n",
    "    # Flatten results\n",
    "    return [item for batch in results for item in batch]\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load datasets\n",
    "    datasets = {\n",
    "        \"train\": load_json_file('Dataset/train.json'),\n",
    "        \"test\": load_json_file('Dataset/test.json'),\n",
    "        \"dev\": load_json_file('Dataset/dev.json')\n",
    "    }\n",
    "\n",
    "    # Process datasets\n",
    "    for key, data in datasets.items():\n",
    "        processed_data = process_dataset_in_batches(data)\n",
    "        \n",
    "        # Update dataset with processed results\n",
    "        for i, item in enumerate(data):\n",
    "            item['text'] = processed_data[i]['text']\n",
    "            item['Başvuru Konusu'] = processed_data[i]['Başvuru Konusu']\n",
    "\n",
    "        # Save processed dataset\n",
    "        save_json_file(data, f'{key}_processed.json')\n",
    "\n",
    "    print(f\"Processing completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Critical error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
