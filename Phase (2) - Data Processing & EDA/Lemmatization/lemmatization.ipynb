{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib numpy datasets logging typing zeyrek nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Instance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from zeyrek import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename='process_preprocessing_debug.log',\n",
    "    filemode='w',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.DEBUG\n",
    ")\n",
    "\n",
    "# Load NLTK Turkish stop words\n",
    "try:\n",
    "    nltk_stop_words = stopwords.words('turkish')\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk_stop_words = stopwords.words('turkish')\n",
    "\n",
    "# Custom stop words\n",
    "custom_stop_words = [\n",
    "    'karar', 'mahkemesi', 'edildiği', 'şöyledir', 'hakkının', 'hukuk',\n",
    "    'hakkında', 'ifade', 'tarihli', 'olarak', 'ceza', 'kararı', 'başvuru',\n",
    "    'başvurucunun', 'üzerine', 'yapılan', 'tarafından', 'kabul', 'verilmiştir',\n",
    "    'kanun', 'tarihinde', 'dava', 'nedeniyle', 'ilişkin', 'maddesinin', 'ilgili',\n",
    "    'başvurucu', 'yer', 'terör', 'sayılı', 'olan', 'ihlal', 'olduğu', 'nin'\n",
    "]\n",
    "\n",
    "# Combine stop words and compile regex\n",
    "all_stop_words = set(nltk_stop_words + custom_stop_words)\n",
    "stop_words_regex = re.compile(r'\\b(?:' + '|'.join(map(re.escape, all_stop_words)) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans the text by removing punctuation, digits, stop words, and converting to lowercase.\"\"\"\n",
    "    logging.debug(f\"Original text: {text}\")\n",
    "    \n",
    "    # Remove digits and numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    logging.debug(f\"After removing digits: {text}\")\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    logging.debug(f\"After removing punctuation: {text}\")\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = stop_words_regex.sub('', text)\n",
    "    logging.debug(f\"After removing stop words: {text}\")\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    logging.debug(f\"After lowercase conversion: {text}\")\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_single_instance():\n",
    "    \"\"\"Process a single instance from the dataset for debugging.\"\"\"\n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = MorphAnalyzer()\n",
    "        \n",
    "        # Load dataset and get first instance\n",
    "        dataset = load_json_file('D:/Github Projects/Machine Learning in Law/Phase (2) - Data Processing & EDA/Dataset - ML Project/Json Format/train.json')\n",
    "        first_instance = dataset[0]\n",
    "        \n",
    "        # Process text fields\n",
    "        for field in ['text', 'Başvuru Konusu']:\n",
    "            print(f\"\\nProcessing {field}:\")\n",
    "            print(\"Original:\", first_instance[field][:200])\n",
    "            \n",
    "            # Clean text\n",
    "            cleaned_text = clean_text(first_instance[field])\n",
    "            print(\"Cleaned:\", cleaned_text[:200])\n",
    "            \n",
    "            # Lemmatize\n",
    "            words = cleaned_text.split()\n",
    "            lemmatized_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                try:\n",
    "                    # Use analyze instead of lemmatize\n",
    "                    analyses = analyzer.analyze(word)\n",
    "                    if analyses:\n",
    "                        # Get the lemma from the first analysis\n",
    "                        lemma = analyses[0].lemma\n",
    "                        lemmatized_words.append(lemma)\n",
    "                        print(f\"Word: {word} -> Lemma: {lemma}\")\n",
    "                    else:\n",
    "                        lemmatized_words.append(word)\n",
    "                        print(f\"Word: {word} -> No analysis found\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error analyzing word '{word}': {str(e)}\")\n",
    "                    lemmatized_words.append(word)\n",
    "                    print(f\"Word: {word} -> Error in analysis\")\n",
    "            \n",
    "            processed_text = ' '.join(lemmatized_words)\n",
    "            print(f\"Final processed {field}:\", processed_text[:200])\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing single instance: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_single_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from zeyrek import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Configure logging (minimal logging for performance)\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename='process_preprocessing_v3.log',\n",
    "    filemode='w',  # Overwrite the log file each run\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.FATAL\n",
    ")\n",
    "\n",
    "# Load NLTK Turkish stop words\n",
    "try:\n",
    "    nltk_stop_words = stopwords.words('turkish')\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk_stop_words = stopwords.words('turkish')\n",
    "\n",
    "# Custom stop words\n",
    "custom_stop_words = [\n",
    "    'karar', 'mahkemesi', 'edildiği', 'şöyledir', 'hakkının', 'hukuk',\n",
    "    'hakkında', 'ifade', 'tarihli', 'olarak', 'ceza', 'kararı', 'başvuru',\n",
    "    'başvurucunun', 'üzerine', 'yapılan', 'tarafından', 'kabul', 'verilmiştir',\n",
    "    'kanun', 'tarihinde', 'dava', 'nedeniyle', 'ilişkin', 'maddesinin', 'ilgili',\n",
    "    'başvurucu', 'yer', 'terör', 'sayılı', 'olan', 'ihlal', 'olduğu', 'nin'\n",
    "]\n",
    "\n",
    "# Combine stop words and compile regex\n",
    "all_stop_words = set(nltk_stop_words + custom_stop_words)\n",
    "stop_words_regex = re.compile(r'\\b(?:' + '|'.join(map(re.escape, all_stop_words)) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Global MorphAnalyzer for worker processes\n",
    "analyzer = None\n",
    "\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json_file(data, file_path):\n",
    "    \"\"\"Save JSON file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans the text by removing punctuation, digits, stop words, and converting to lowercase.\"\"\"\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove digits and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = stop_words_regex.sub('', text)  # Remove stop words\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatizes a single text.\"\"\"\n",
    "    global analyzer\n",
    "    cleaned_text = clean_text(text)\n",
    "    words = cleaned_text.split()\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemmas = analyzer.lemmatize(word)\n",
    "        if lemmas:\n",
    "            lemmatized_words.append(lemmas[0][1][0])  # Append the first lemma\n",
    "        else:\n",
    "            lemmatized_words.append(word)  # Fallback to the original word\n",
    "    #print(' '.join(lemmatized_words))\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "def process_texts_batch(batch):\n",
    "    \"\"\"Processes a batch of texts for lemmatization using the global analyzer.\"\"\"\n",
    "    processed_batch = []\n",
    "    for item in batch:\n",
    "        processed_item = {}\n",
    "        for field in ['text', 'Başvuru Konusu']:\n",
    "            processed_item[field] = lemmatize_text(item[field])\n",
    "        processed_batch.append(processed_item)\n",
    "    return processed_batch\n",
    "\n",
    "\n",
    "def worker_init():\n",
    "    \"\"\"Initializes the global MorphAnalyzer in workers.\"\"\"\n",
    "    global analyzer\n",
    "    analyzer = MorphAnalyzer()\n",
    "\n",
    "\n",
    "def process_dataset_in_batches(dataset, batch_size=100):\n",
    "    \"\"\"Processes the dataset in batches using multiprocessing.\"\"\"\n",
    "    with Pool(processes=cpu_count(), initializer=worker_init) as pool:\n",
    "        # Split dataset into batches\n",
    "        batches = [dataset[i:i + batch_size] for i in range(0, len(dataset), batch_size)]\n",
    "        \n",
    "        # Parallel processing\n",
    "        results = list(tqdm(pool.imap(process_texts_batch, batches),\n",
    "                            total=len(batches), desc=\"Processing Dataset\"))\n",
    "\n",
    "    # Flatten results\n",
    "    return [item for batch in results for item in batch]\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load datasets\n",
    "    datasets = {\n",
    "        \"train\": load_json_file('D:/Github Projects/Machine Learning in Law/Phase (2) - Data Processing & EDA/Dataset - ML Project/Json Format/train.json'),\n",
    "        \"test\": load_json_file('D:/Github Projects/Machine Learning in Law/Phase (2) - Data Processing & EDA/Dataset - ML Project/Json Format/test.json'),\n",
    "        \"dev\": load_json_file('D:/Github Projects/Machine Learning in Law/Phase (2) - Data Processing & EDA/Dataset - ML Project/Json Format/dev.json')\n",
    "    }\n",
    "\n",
    "    # Process datasets\n",
    "    for key, data in datasets.items():\n",
    "        processed_data = process_dataset_in_batches(data)\n",
    "        \n",
    "        # Update dataset with processed results\n",
    "        for i, item in enumerate(data):\n",
    "            item['text'] = processed_data[i]['text']\n",
    "            item['Başvuru Konusu'] = processed_data[i]['Başvuru Konusu']\n",
    "\n",
    "        # Save processed dataset\n",
    "        save_json_file(data, f'{key}_processed.json')\n",
    "\n",
    "    print(f\"Processing completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Critical error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
